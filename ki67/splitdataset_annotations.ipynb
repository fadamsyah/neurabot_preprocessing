{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from utils import convert2coco, dataset_split, dataset_analysis\n",
    "from utils import coco_to_img2annots, img2annots_to_coco, save_json_file\n",
    "from utils import check_image_id_duplication, check_annotation_id_duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_num_objects(annotations, num_objects_split=120):\n",
    "    img2annots = coco_to_img2annots(annotations)\n",
    "    \n",
    "    img2annots1 = {\n",
    "        'type': img2annots['type'],\n",
    "        'categories': img2annots['categories'],\n",
    "        'img2annots': {}\n",
    "    }\n",
    "    \n",
    "    img2annots2 = deepcopy(img2annots1)\n",
    "    \n",
    "    for key, val in img2annots['img2annots'].items():\n",
    "        num_objects = 0\n",
    "        for _, no in val['num_objects'].items():\n",
    "            num_objects = num_objects + no\n",
    "        \n",
    "        if num_objects >= num_objects_split:\n",
    "            img2annots1['img2annots'][key] = val\n",
    "        else:\n",
    "            img2annots2['img2annots'][key] = val\n",
    "        \n",
    "    # print(len(img2annots1['img2annots']))\n",
    "    # print(len(img2annots2['img2annots']))\n",
    "    \n",
    "    return img2annots_to_coco(img2annots1), img2annots_to_coco(img2annots2)\n",
    "\n",
    "def concatenate_2_coco_annotations(annotations_1, annotations_2):\n",
    "    ########## IMPORTANT ##########\n",
    "    # Assumption: inputs have exactly the same type and categories\n",
    "    \n",
    "    annotations = deepcopy(annotations_1)\n",
    "    for key in ['images', 'annotations']:\n",
    "        for val in annotations_2[key]:\n",
    "            annotations[key].append(val)\n",
    "            \n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = './data/ori'\n",
    "annotations = convert2coco(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_split_1, annotations_split_2 = split_by_num_objects(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best error: 0.0011583955693151798\n",
      "The best error: 0.0007074338279419634\n",
      "The best error: 0.0005309330031942209\n",
      "The best error: 0.0003345984785767271\n",
      "The best error: 0.00015668930596868922\n",
      "The best error: 4.170811961990254e-05\n",
      "The best error: 4.047713685417575e-05\n",
      "The best error: 3.370437759060894e-05\n",
      "The best error: 2.7202927936513847e-05\n",
      "The best error: 2.2908706461815647e-05\n",
      "The best error: 0.0030899645694929135\n",
      "The best error: 0.00299280673003211\n",
      "The best error: 0.0012331221015432624\n",
      "The best error: 0.0004846979143656389\n",
      "The best error: 0.0004468568113179473\n",
      "The best error: 0.0003680497345411491\n",
      "The best error: 0.0003520994616082913\n",
      "The best error: 0.00034519353350321035\n",
      "The best error: 0.0002561975663368075\n",
      "The best error: 8.874325999029039e-05\n",
      "The best error: 4.221448566508577e-05\n",
      "The best error: 3.407378795580049e-05\n"
     ]
    }
   ],
   "source": [
    "split_dictionary = {\n",
    "    'train': 0.60,\n",
    "    'val': 0.20,\n",
    "    'test': 0.20\n",
    "}\n",
    "\n",
    "ann_split_1 = dataset_split(annotations_split_1, split_dictionary, 20000)\n",
    "ann_split_2 = dataset_split(annotations_split_2, split_dictionary, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "num_images          89\n",
      "num_objects      16228\n",
      "-----------------------------------\n",
      "num_images on each set\n",
      "\n",
      "train          54    0.607\n",
      "val            18    0.202\n",
      "test           17    0.191\n",
      "-----------------------------------\n",
      "num_objects on each set\n",
      "\n",
      "train        9747    0.601\n",
      "val          3223    0.199\n",
      "test         3258    0.201\n",
      "-----------------------------------\n",
      "Category: 1\n",
      "\n",
      "train        4482    0.603\n",
      "val          1468    0.198\n",
      "test         1481    0.199\n",
      "-----------------------------------\n",
      "Category: 2\n",
      "\n",
      "train        5265    0.598\n",
      "val          1755    0.199\n",
      "test         1777    0.202\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "results_split_1 = dataset_analysis(ann_split_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "num_images          91\n",
      "num_objects       6911\n",
      "-----------------------------------\n",
      "num_images on each set\n",
      "\n",
      "train          55    0.604\n",
      "val            19    0.209\n",
      "test           17    0.187\n",
      "-----------------------------------\n",
      "num_objects on each set\n",
      "\n",
      "train        4166    0.603\n",
      "val          1380    0.200\n",
      "test         1365    0.198\n",
      "-----------------------------------\n",
      "Category: 1\n",
      "\n",
      "train        2361    0.604\n",
      "val           779    0.199\n",
      "test          767    0.196\n",
      "-----------------------------------\n",
      "Category: 2\n",
      "\n",
      "train        1805    0.601\n",
      "val           601    0.200\n",
      "test          598    0.199\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "results_split_2 = dataset_analysis(ann_split_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "num_images         180\n",
      "num_objects      23139\n",
      "-----------------------------------\n",
      "num_images on each set\n",
      "\n",
      "train         109    0.606\n",
      "val            37    0.206\n",
      "test           34    0.189\n",
      "-----------------------------------\n",
      "num_objects on each set\n",
      "\n",
      "train       13913    0.601\n",
      "val          4603    0.199\n",
      "test         4623    0.200\n",
      "-----------------------------------\n",
      "Category: 1\n",
      "\n",
      "train        6843    0.604\n",
      "val          2247    0.198\n",
      "test         2248    0.198\n",
      "-----------------------------------\n",
      "Category: 2\n",
      "\n",
      "train        7070    0.599\n",
      "val          2356    0.200\n",
      "test         2375    0.201\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "anns = {}\n",
    "for set_name in ['train', 'val', 'test']:\n",
    "    anns[set_name] = concatenate_2_coco_annotations(ann_split_1[set_name], ann_split_2[set_name])\n",
    "results = dataset_analysis(anns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "set_name: train \n",
      "\n",
      "image_id_duplication       False\n",
      "annotation_id_duplication  False\n",
      "----------------------------------------\n",
      "set_name: val \n",
      "\n",
      "image_id_duplication       False\n",
      "annotation_id_duplication  False\n",
      "----------------------------------------\n",
      "set_name: test \n",
      "\n",
      "image_id_duplication       False\n",
      "annotation_id_duplication  False\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Save and check duplication\n",
    "print('-'*40)\n",
    "for key, val in anns.items():\n",
    "    save_json_file(val, f'data/instances_{key}.json')\n",
    "    print('set_name:', key,'\\n')\n",
    "    print('image_id_duplication', ' '*5, check_image_id_duplication(val['images']))\n",
    "    print('annotation_id_duplication', '', check_annotation_id_duplication(val['annotations']))\n",
    "    print('-'*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
