{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from utils import convert2coco, dataset_split, dataset_analysis\n",
    "from utils import coco_to_img2annots, img2annots_to_coco, save_json_file\n",
    "from utils import check_image_id_duplication, check_annotation_id_duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_num_objects(annotations, num_objects_split=120):\n",
    "    img2annots = coco_to_img2annots(annotations)\n",
    "    \n",
    "    img2annots1 = {\n",
    "        'type': img2annots['type'],\n",
    "        'categories': img2annots['categories'],\n",
    "        'img2annots': {}\n",
    "    }\n",
    "    \n",
    "    img2annots2 = deepcopy(img2annots1)\n",
    "    \n",
    "    for key, val in img2annots['img2annots'].items():\n",
    "        num_objects = 0\n",
    "        for _, no in val['num_objects'].items():\n",
    "            num_objects = num_objects + no\n",
    "        \n",
    "        if num_objects >= num_objects_split:\n",
    "            img2annots1['img2annots'][key] = val\n",
    "        else:\n",
    "            img2annots2['img2annots'][key] = val\n",
    "        \n",
    "    # print(len(img2annots1['img2annots']))\n",
    "    # print(len(img2annots2['img2annots']))\n",
    "    \n",
    "    return img2annots_to_coco(img2annots1), img2annots_to_coco(img2annots2)\n",
    "\n",
    "def concatenate_2_coco_annotations(annotations_1, annotations_2):\n",
    "    ########## IMPORTANT ##########\n",
    "    # Assumption: inputs have exactly the same type and categories\n",
    "    \n",
    "    annotations = deepcopy(annotations_1)\n",
    "    for key in ['images', 'annotations']:\n",
    "        for val in annotations_2[key]:\n",
    "            annotations[key].append(val)\n",
    "            \n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = './data/ori'\n",
    "annotations = convert2coco(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_split_1, annotations_split_2 = split_by_num_objects(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best error: 0.0025285820459238096\n",
      "The best error: 0.001728383055635808\n",
      "The best error: 0.0008509389410633236\n",
      "The best error: 0.0007904229430527673\n",
      "The best error: 0.0006755372506618615\n",
      "The best error: 0.0005724400321431754\n",
      "The best error: 0.00024343343452798446\n",
      "The best error: 0.00010672960672460979\n",
      "The best error: 8.630289129719244e-05\n",
      "The best error: 7.303264618028857e-05\n",
      "The best error: 6.406480665843996e-05\n",
      "The best error: 2.91151516095374e-05\n",
      "The best error: 1.7847396297138352e-05\n",
      "The best error: 1.6065037539174714e-05\n",
      "The best error: 0.0026950399164626672\n",
      "The best error: 0.002634227667185885\n",
      "The best error: 0.0020762380401282356\n",
      "The best error: 0.0008450298707356742\n",
      "The best error: 0.0006081089980456688\n",
      "The best error: 0.0005525958410105324\n",
      "The best error: 0.00044268733609703734\n",
      "The best error: 0.00040963313325720634\n",
      "The best error: 0.00016948587714615793\n",
      "The best error: 0.00012890769545038813\n",
      "The best error: 0.0001243975780364595\n",
      "The best error: 0.00010902651931936703\n",
      "The best error: 8.829908163635165e-06\n"
     ]
    }
   ],
   "source": [
    "split_dictionary = {\n",
    "    'train': 0.60,\n",
    "    'val': 0.20,\n",
    "    'test': 0.20\n",
    "}\n",
    "\n",
    "ann_split_1 = dataset_split(annotations_split_1, split_dictionary, 10000)\n",
    "ann_split_2 = dataset_split(annotations_split_2, split_dictionary, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "num_images          89\n",
      "num_objects      16228\n",
      "-----------------------------------\n",
      "num_images on each set\n",
      "\n",
      "train          54    0.607\n",
      "val            18    0.202\n",
      "test           17    0.191\n",
      "-----------------------------------\n",
      "num_objects on each set\n",
      "\n",
      "train        9744    0.600\n",
      "val          3273    0.202\n",
      "test         3211    0.198\n",
      "-----------------------------------\n",
      "Category: 1\n",
      "\n",
      "train        4457    0.600\n",
      "val          1500    0.202\n",
      "test         1474    0.198\n",
      "-----------------------------------\n",
      "Category: 2\n",
      "\n",
      "train        5287    0.601\n",
      "val          1773    0.202\n",
      "test         1737    0.197\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "results_split_1 = dataset_analysis(ann_split_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "num_images          91\n",
      "num_objects       6911\n",
      "-----------------------------------\n",
      "num_images on each set\n",
      "\n",
      "train          55    0.604\n",
      "val            19    0.209\n",
      "test           17    0.187\n",
      "-----------------------------------\n",
      "num_objects on each set\n",
      "\n",
      "train        4147    0.600\n",
      "val          1380    0.200\n",
      "test         1384    0.200\n",
      "-----------------------------------\n",
      "Category: 1\n",
      "\n",
      "train        2343    0.600\n",
      "val           776    0.199\n",
      "test          788    0.202\n",
      "-----------------------------------\n",
      "Category: 2\n",
      "\n",
      "train        1804    0.601\n",
      "val           604    0.201\n",
      "test          596    0.198\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "results_split_2 = dataset_analysis(ann_split_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "num_images         180\n",
      "num_objects      23139\n",
      "-----------------------------------\n",
      "num_images on each set\n",
      "\n",
      "train         109    0.606\n",
      "val            37    0.206\n",
      "test           34    0.189\n",
      "-----------------------------------\n",
      "num_objects on each set\n",
      "\n",
      "train       13891    0.600\n",
      "val          4653    0.201\n",
      "test         4595    0.199\n",
      "-----------------------------------\n",
      "Category: 1\n",
      "\n",
      "train        6800    0.600\n",
      "val          2276    0.201\n",
      "test         2262    0.200\n",
      "-----------------------------------\n",
      "Category: 2\n",
      "\n",
      "train        7091    0.601\n",
      "val          2377    0.201\n",
      "test         2333    0.198\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "anns = {}\n",
    "for set_name in ['train', 'val', 'test']:\n",
    "    anns[set_name] = concatenate_2_coco_annotations(ann_split_1[set_name], ann_split_2[set_name])\n",
    "results = dataset_analysis(anns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "set_name: train \n",
      "\n",
      "image_id_duplication       False\n",
      "annotation_id_duplication  False\n",
      "----------------------------------------\n",
      "set_name: val \n",
      "\n",
      "image_id_duplication       False\n",
      "annotation_id_duplication  False\n",
      "----------------------------------------\n",
      "set_name: test \n",
      "\n",
      "image_id_duplication       False\n",
      "annotation_id_duplication  False\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Save and check duplication\n",
    "print('-'*40)\n",
    "for key, val in anns.items():\n",
    "    save_json_file(val, f'data/instances_{key}.json')\n",
    "    print('set_name:', key,'\\n')\n",
    "    print('image_id_duplication', ' '*5, check_image_id_duplication(val['images']))\n",
    "    print('annotation_id_duplication', '', check_annotation_id_duplication(val['annotations']))\n",
    "    print('-'*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
